# ğŸš€ Ollama + NewRoots - Complete Setup Guide

## Status: âœ… READY TO USE

Your NewRoots backend is fully configured for **Ollama with multilingual AI support**.

---

## What Has Been Set Up

### 1. **Backend Integration** âœ…
- [OllamaService.java](src/main/java/com/newroots/backend/service/OllamaService.java) - Handles all Ollama API calls
- [ChatService.java](src/main/java/com/newroots/backend/service/ChatService.java) - Routes chat requests to Ollama
- [ChatController.java](src/main/java/com/newroots/backend/controller/ChatController.java) - REST endpoint at `/chat`

### 2. **Configuration** âœ…
- [application.properties](src/main/resources/application.properties) - Pre-configured with:
  - `ollama.base-url=http://localhost:11434` (default Ollama port)
  - `ollama.model=mistral` (multilingual model)
  - Server port: `8081`

### 3. **Multilingual Support** âœ…
Enhanced `buildPrompt()` method now supports:
- ğŸ‡¬ğŸ‡§ English (default)
- ğŸ‡ªğŸ‡¸ Spanish
- ğŸ‡«ğŸ‡· French
- ğŸ‡µğŸ‡¹ Portuguese
- ğŸ‡©ğŸ‡ª German
- ğŸ‡®ğŸ‡¹ Italian
- ğŸ‡¨ğŸ‡³ Chinese (Simplified)
- ğŸ‡»ğŸ‡³ Vietnamese
- ğŸ‡°ğŸ‡· Korean
- ğŸ‡µğŸ‡­ Tagalog/Filipino
- ğŸ‡¸ğŸ‡¦ Arabic

### 4. **Frontend Integration** âœ…
- [ChatInterface.tsx](frontend/app/components/ChatInterface.tsx) - Sends language parameter with messages
- [vite.config.ts](vite.config.ts) - Proxies `/chat` requests to backend

### 5. **Documentation** âœ…
- [OLLAMA_SETUP.md](./OLLAMA_SETUP.md) - Detailed setup guide
- [OLLAMA_QUICKSTART.md](./OLLAMA_QUICKSTART.md) - Quick start
- [OLLAMA_TEST.md](./OLLAMA_TEST.md) - **NEW** Comprehensive test & verification guide

---

## How to Run Ollama + NewRoots

You need **3 terminals** running simultaneously:

### Terminal 1: Start Ollama Server
```bash
ollama serve
```

### Terminal 2: Start Backend
```bash
cd /Users/trishnguyen/newroots
./mvnw spring-boot:run -Dspring-boot.run.arguments=--server.port=8081
```

### Terminal 3: Start Frontend
```bash
cd /Users/trishnguyen/newroots
npm run dev
```

Then open: **http://localhost:5174/**

---

## Why Ollama? Key Features

| Feature | Ollama | OpenAI |
|---------|--------|--------|
| **Cost** | Free | $$ per request |
| **Privacy** | Local (100% private) | Cloud-based |
| **Internet** | Works offline | Requires internet |
| **Speed** | Fast (local) | Faster (larger models) |
| **Customization** | Full control | Limited |
| **Multilingual** | Yes (native) | Yes |
| **Setup** | Simple | API key needed |

**For an immigrant resource app, Ollama is perfect because:**
1. âœ… Multilingual capabilities (Spanish, Vietnamese, Chinese, etc.)
2. âœ… No data sent to external servers (privacy)
3. âœ… No API keys required
4. âœ… Works offline
5. âœ… Free to use

---

## Testing Checklist

Before deploying, verify:

```bash
# 1. Ollama is running
curl http://localhost:11434/api/tags

# 2. Model is installed
ollama list

# 3. Backend starts
./mvnw spring-boot:run -Dspring-boot.run.arguments=--server.port=8081

# 4. Test English chat
curl -X POST http://localhost:8081/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"What is health insurance?","language":"en"}'

# 5. Test Spanish chat (multilingual)
curl -X POST http://localhost:8081/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Â¿QuÃ© es el seguro de salud?","language":"es"}'

# 6. Frontend works
npm run dev
# Open http://localhost:5174/ and test chat interface
```

See [OLLAMA_TEST.md](./OLLAMA_TEST.md) for detailed testing guide.

---

## Model Recommendations

| Use Case | Model | Command |
|----------|-------|---------|
| **Default** (Recommended) | mistral | `ollama pull mistral` |
| Fast & Small | neural-chat | `ollama pull neural-chat` |
| Best Quality | llama2 | `ollama pull llama2` |
| Code-focused | openchat | `ollama pull openchat` |

Currently configured: **mistral** (perfect balance of speed and quality)

---

## Troubleshooting

### Problem: "Ollama connection failed"
**Solution**: Make sure `ollama serve` is running in Terminal 1

### Problem: Model not found
**Solution**: Run `ollama pull mistral` to download the model

### Problem: Very slow responses
**Solution**: Switch to smaller model `ollama pull neural-chat`

### Problem: Port already in use
**Solution**: Use different port: `./mvnw spring-boot:run -Dspring-boot.run.arguments=--server.port=8082`

See [OLLAMA_TEST.md](./OLLAMA_TEST.md) for more troubleshooting.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Frontend (React + Vite)           â”‚
â”‚      Sends: {message, language}         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ POST /chat
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Backend (Spring Boot, Port 8081)     â”‚
â”‚      ChatController â†’ ChatService        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP POST /api/generate
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Server (Port 11434, Local)     â”‚
â”‚   Handles: Multilingual text generation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        LLM Model (Mistral 7B)           â”‚
â”‚   Generates responses in requested     â”‚
â”‚        language with immigrant         â”‚
â”‚         context prompts                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Next Steps

1. âœ… **Install Ollama** from ollama.ai
2. âœ… **Pull a model**: `ollama pull mistral`
3. âœ… **Start Ollama**: `ollama serve`
4. âœ… **Test backend**: Follow Terminal 2 & 3 instructions
5. âœ… **Verify in browser**: Open http://localhost:5174/ and test chat
6. ğŸ“‹ **Run full test suite**: See [OLLAMA_TEST.md](./OLLAMA_TEST.md)

---

## Code Files

Key files for reference:

- **Service Logic**: [OllamaService.java](src/main/java/com/newroots/backend/service/OllamaService.java)
  - Handles API calls to Ollama
  - Includes multilingual system prompts
  - Error handling with helpful messages

- **Chat Routing**: [ChatService.java](src/main/java/com/newroots/backend/service/ChatService.java)
  - Routes requests to OllamaService

- **REST Endpoint**: [ChatController.java](src/main/java/com/newroots/backend/controller/ChatController.java)
  - Endpoint: `POST /chat`
  - Accepts: `{message: string, language: string}`
  - Returns: `{reply: string}`

- **Configuration**: [application.properties](src/main/resources/application.properties)
  - Ollama URL: `http://localhost:11434`
  - Model: `mistral`
  - Server port: `8081`

---

## Production Deployment

When deploying to production:

1. **Ensure Ollama runs on production server**:
   ```bash
   ollama serve &  # background process or systemd service
   ```

2. **Secure Ollama** (don't expose to internet):
   ```bash
   # Only allow local connections
   firewall-cmd --add-port=11434/tcp --permanent
   # Or use reverse proxy that only backend can reach
   ```

3. **Monitor resources**:
   - Ollama uses significant RAM (8GB+ recommended)
   - Monitor CPU usage during inference
   - Consider load balancing if many users

4. **Scale options**:
   - Use smaller models for faster responses
   - Run multiple Ollama instances with load balancer
   - Consider GPUs for faster inference

---

## Questions?

Refer to:
- Detailed setup: [OLLAMA_SETUP.md](./OLLAMA_SETUP.md)
- Quick start: [OLLAMA_QUICKSTART.md](./OLLAMA_QUICKSTART.md)
- Full testing: [OLLAMA_TEST.md](./OLLAMA_TEST.md)
- Official Ollama: https://ollama.ai

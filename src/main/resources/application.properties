server.port=8081

# Ollama Configuration (Local LLM - Recommended)
# Make sure Ollama is running: ollama serve
# And pull a model: ollama pull mistral
ollama.base-url=http://localhost:11434
ollama.model=qwen2.5:3b

# OpenAI Configuration (Alternative - if you want to use OpenAI instead)
# openai.api.key should NOT be committed. Set it via environment variable instead.
# Example: export OPENAI_API_KEY="sk-..."
# Spring will map `OPENAI_API_KEY` -> `openai.api.key` property automatically.
# openai.model=gpt-4o-mini
